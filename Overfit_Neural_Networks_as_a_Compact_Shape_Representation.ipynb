{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Overfit Neural Networks as a Compact Shape Representation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUiejv5QV4_N"
      },
      "source": [
        "import os\n",
        "import time\n",
        "from torch import nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from SDFSampler import PointSampler, MeshLoader\n",
        "\n",
        "class NeuralImplicit:\n",
        "  def __init__(self, N = 8, H = 32):\n",
        "    self.model = self.OverfitSDF(N, H)\n",
        "    self.epochs = 100\n",
        "    self.lr = 1e-4\n",
        "    self.batch_size = 64\n",
        "    self.log_iterations = 1000\n",
        "\n",
        "  # Supported mesh file formats are .obj and .stl\n",
        "  # Sampler selects oversample_ratio * num_sample points around the mesh, keeping only num_sample most\n",
        "  # important points as determined by the importance metric\n",
        "  def encode(self, mesh_file, num_samples=1000000, oversample_ratio = 30, early_stop=None, verbose=True):\n",
        "    dataset = self.MeshDataset(mesh_file, num_samples, oversample_ratio)\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    self.model.to(device)\n",
        "\n",
        "    loss_func = nn.L1Loss(reduction='mean')\n",
        "    optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "\n",
        "    for e in range(self.epochs):\n",
        "      epoch_loss = 0\n",
        "      self.model.train()\n",
        "      count = 0\n",
        "\n",
        "      for batch_idx, (x_train, y_train) in enumerate(dataloader):\n",
        "        x_train, y_train = x_train.to(device), y_train.to(device)\n",
        "        count += self.batch_size\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred = self.model(x_train)\n",
        "\n",
        "        loss = loss_func(y_pred, y_train)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        if (verbose and ((batch_idx+1) % self.log_iterations == 0)):\n",
        "          msg = '{}\\tEpoch: {}:\\t[{}/{}]\\tepoch_loss: {:.6f}\\tloss: {:.6f}'.format(\n",
        "              time.ctime(),\n",
        "              e + 1,\n",
        "              count,\n",
        "              len(dataset),\n",
        "              epoch_loss / (batch_idx + 1),\n",
        "              loss)\n",
        "          print(msg)\n",
        "\n",
        "      if (early_stop and epoch_loss < early_stop):\n",
        "        break\n",
        "    \n",
        "    model_file = os.path.dirname(mesh_file) + \"/\" + os.path.splitext(os.path.basename(mesh_file))[0] + \".pth\"\n",
        "    torch.save(self.model.state_dict(), model_file)\n",
        "    \n",
        "  # The actual network here is just a simple MLP\n",
        "  class OverfitSDF(nn.Module):\n",
        "    def __init__(self, N, H):\n",
        "      super().__init__()\n",
        "      assert(N > 0)\n",
        "      assert(H > 0)\n",
        "\n",
        "      # Original paper uses ReLU but I found this lead to dying ReLU issues\n",
        "      # with negative coordinates. Perhaps was not an issue with original paper's\n",
        "      # dataset?\n",
        "      net = [nn.Linear(3, H), nn.LeakyReLU(0.1)]\n",
        "      \n",
        "      for i in range(0,N):\n",
        "        net += [nn.Linear(H, H), nn.LeakyReLU(0.1)]\n",
        "\n",
        "      net += [nn.Linear(H,1), nn.LeakyReLU(0.1)]\n",
        "      self.model = nn.Sequential(*net)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.model(x)\n",
        "      output = torch.tanh(x)\n",
        "      return output\n",
        "\n",
        "  # Dataset generates data from the mesh file using the SDFSampler library on CPU\n",
        "  # Moving data generation to GPU should speed up this process significantly\n",
        "  class MeshDataset(Dataset):\n",
        "    def __init__(self, mesh_file, num_samples, oversample_ratio):\n",
        "      print(\"Loading \" + mesh_file, flush=True)\n",
        "      time.sleep(0.1)\n",
        "\n",
        "      vertices, faces = MeshLoader.read(mesh_file)\n",
        "      print(\"Loaded mesh\", flush=True)\n",
        "      time.sleep(0.1)\n",
        "\n",
        "      sampler = PointSampler(vertices, faces)\n",
        "      self.pts = sampler.sample(num_samples, oversample_ratio)\n",
        "      print(\"Sampled \" + str(len(self)) + \" points\", flush=True)\n",
        "      time.sleep(0.1)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      return torch.from_numpy(self.pts[0][index,:]), torch.tensor([self.pts[1][index]])\n",
        "\n",
        "    def __len__(self):\n",
        "      return self.pts[0].shape[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7IRG04mT_Cj",
        "outputId": "a5d6f4cc-c445-4bb1-9ce1-b1e92f013e99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "bunny = NeuralImplicit()\n",
        "bunny.encode(\"bunny.stl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT5BIpQswfH1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}